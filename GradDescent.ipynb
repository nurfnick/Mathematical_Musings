{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled63.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMj40dxy+N5sI80aelBDuZ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurfnick/Mathematical_Musings/blob/main/GradDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7sKvdiswGte"
      },
      "source": [
        "# Gradient Descent for Training Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou6LW0izwUtq"
      },
      "source": [
        "In multiple linear regression, the goal is to minimize the sum of the squared errors.\n",
        "\n",
        "$$ RSS = \\sum_{i=1}^n \\left(y_i-\\hat y_i\\right)^2 = (\\vec y - X \\vec \\beta)^T(\\vec y - X\\vec \\beta) $$\n",
        "\n",
        "This first equation is in the traditional format (where $\\hat y$ is the prediction $\\vec x \\cdot \\vec \\beta$) and the second in a matrix format.\n",
        "\n",
        "We know the solution to this problem analytically, $\\hat \\beta = \\left(X^TX\\right)^{-1}X^T\\vec y$.  What I am interested in is if I can code to find an approximation of this solution by following the gradient descent.\n",
        "\n",
        "The gradient descent is an iterative process where by we initialize a solution (for $\\beta$) and update it by using $\\beta - w \\nabla RSS$ where $w$ is some user chosen weight and \n",
        "$$\n",
        "\\nabla RSS = -2X^T\\left(\\vec y - X\\vec \\beta\\right)\n",
        "$$\n",
        "\n",
        "Okay enough with the theory, let's get our hands dirty with some coding!  I'll start with some data and show the solution for $\\hat \\beta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNPItba_wF3e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pa\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "df = pa.read_csv('https://raw.githubusercontent.com/nurfnick/Applied_Stats_Jupyter_Notebooks/master/blues.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Rx89yAwsz9P0",
        "outputId": "a355b97c-5d1b-441f-dbd8-898cf7a4cb7e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rk</th>\n",
              "      <th>Player</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>Yrs</th>\n",
              "      <th>Lg</th>\n",
              "      <th>GP</th>\n",
              "      <th>G</th>\n",
              "      <th>A</th>\n",
              "      <th>PTS</th>\n",
              "      <th>+/-</th>\n",
              "      <th>PIM</th>\n",
              "      <th>EV</th>\n",
              "      <th>PP</th>\n",
              "      <th>SH</th>\n",
              "      <th>GW</th>\n",
              "      <th>EV.1</th>\n",
              "      <th>PP.1</th>\n",
              "      <th>SH.1</th>\n",
              "      <th>S</th>\n",
              "      <th>S%</th>\n",
              "      <th>TOI</th>\n",
              "      <th>ATOI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Bruce Affleck\\afflebr01</td>\n",
              "      <td>1975</td>\n",
              "      <td>1979</td>\n",
              "      <td>5</td>\n",
              "      <td>NHL</td>\n",
              "      <td>274</td>\n",
              "      <td>14</td>\n",
              "      <td>65</td>\n",
              "      <td>79</td>\n",
              "      <td>-81</td>\n",
              "      <td>86</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>363</td>\n",
              "      <td>3.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Kenny Agostino\\agostke01</td>\n",
              "      <td>2017</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>NHL</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>5.9</td>\n",
              "      <td>89.0</td>\n",
              "      <td>12:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Glenn Anderson*\\andergl01</td>\n",
              "      <td>1995</td>\n",
              "      <td>1996</td>\n",
              "      <td>2</td>\n",
              "      <td>NHL</td>\n",
              "      <td>51</td>\n",
              "      <td>14</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "      <td>-2</td>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>15.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Perry Anderson\\anderpe01</td>\n",
              "      <td>1982</td>\n",
              "      <td>1985</td>\n",
              "      <td>4</td>\n",
              "      <td>NHL</td>\n",
              "      <td>144</td>\n",
              "      <td>22</td>\n",
              "      <td>18</td>\n",
              "      <td>40</td>\n",
              "      <td>-14</td>\n",
              "      <td>355</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>13.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Ron Anderson\\anderro01</td>\n",
              "      <td>1970</td>\n",
              "      <td>1970</td>\n",
              "      <td>1</td>\n",
              "      <td>NHL</td>\n",
              "      <td>59</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>36</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>8.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rk                     Player  From    To  Yrs  ... SH.1    S    S%   TOI   ATOI\n",
              "0   1    Bruce Affleck\\afflebr01  1975  1979    5  ...    3  363   3.9   NaN    NaN\n",
              "1   2   Kenny Agostino\\agostke01  2017  2017    1  ...    0   17   5.9  89.0  12:47\n",
              "2   3  Glenn Anderson*\\andergl01  1995  1996    2  ...    0   89  15.7   NaN    NaN\n",
              "3   4   Perry Anderson\\anderpe01  1982  1985    4  ...    0  168  13.1   NaN    NaN\n",
              "4   5     Ron Anderson\\anderro01  1970  1970    1  ...    0  107   8.4   NaN    NaN\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc-0LQmr0GeI"
      },
      "source": [
        "This dataset is (most?) of the players for my favorite hockey team.  Let's try to predict **GP** from **G**, **A**, and **+/-**.  If you aren't familiar with these you can gain some content knowledge about hockey [here](https://en.wikipedia.org/wiki/Ice_hockey)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9waEMuBz_O_"
      },
      "source": [
        "X= np.array(df[['G', 'A', '+/-']])\n",
        "y = np.array(df['GP'])\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRAQIjwy3CT4"
      },
      "source": [
        "X = np.insert(X,3,1,axis = 1)#Adding 1's to get the intercept too!"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pTx6OjG2UWN"
      },
      "source": [
        "By the above equation..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMFaKVSg01g3",
        "outputId": "fcac7d7a-fe3c-4673-c220-b5c879649e1b"
      },
      "source": [
        "XX = np.dot(X.transpose(),X)\n",
        "z = np.linalg.inv(XX)\n",
        "w = z@X.transpose()\n",
        "np.dot(w,y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.32673324,  1.79968341,  0.21442554, 50.11143093])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl06il0KAdpk"
      },
      "source": [
        "Let's double check with a built-in call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2Igsko1jxo",
        "outputId": "9a4ff7bf-c979-405a-dbc0-8698b0d14a1a"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "linreg = LinearRegression()#define it as a regular linear regression\n",
        "linreg.fit(X,y)#This fits it!\n",
        "linreg.coef_"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.32673324, 1.79968341, 0.21442554, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibEU6KPNAkX1"
      },
      "source": [
        "The final 0 is just because it casts the intercept differently then we have, it is displayed next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAlLXEkl2hBF",
        "outputId": "13829333-966a-434a-a97f-10de8e4c077c"
      },
      "source": [
        "linreg.intercept_"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.11143092932841"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmiNH7xaAsPT"
      },
      "source": [
        "So we know the answer, great, let's try to estimate it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzfwhzoG44n7",
        "outputId": "85a8badd-81df-4bff-ffe1-cde464b1ef15"
      },
      "source": [
        "b = np.random.normal(0,5,4)#intitialize\n",
        "w = 0.0001 #a weight for the descent\n",
        "def grad(b):\n",
        "  return -2/len(y)*(X.transpose())@(y-X@b)\n",
        "\n",
        "for i in range(100000):\n",
        "  b = b - w*grad(b)\n",
        "  \n",
        "b"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.32673322,  1.79968351,  0.2144255 , 50.11141972])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrbaBDPZYLo4"
      },
      "source": [
        "This is working only after much consternation.  I really thought I would see convergence quickly!  Let's revisit this code and show how it is doing in a few key places"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeuTRCiZYcls",
        "outputId": "d22543aa-d00b-495c-9fa2-6c575c939fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "b = np.random.normal(0,5,4)#intitialize\n",
        "w = 0.0001 #a weight for the descent\n",
        "\n",
        "for i in range(50000):\n",
        "  b = b - w*grad(b)\n",
        "  if i % 1000 == 0:\n",
        "    print(b)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.23413989 2.56654757 6.24444646 6.65501249]\n",
            "[ 0.23842688  2.11006871  0.08644685 12.80332648]\n",
            "[ 0.25088538  2.06627869  0.10450244 18.06685743]\n",
            "[ 0.26158621  2.02866669  0.1200107  22.58779483]\n",
            "[ 0.27077734  1.99636109  0.133331   26.47090583]\n",
            "[ 0.27867175  1.96861326  0.14477205 29.80617675]\n",
            "[ 0.2854524   1.94478017  0.15459895 32.67089838]\n",
            "[ 0.29127642  1.92430952  0.16303945 35.13145714]\n",
            "[ 0.29627876  1.90672692  0.17028915 37.24487343]\n",
            "[ 0.30057537  1.89162492  0.17651603 39.06012308]\n",
            "[ 0.30426579  1.87865356  0.18186441 40.61927228]\n",
            "[ 0.30743556  1.86751223  0.18645822 41.95845237]\n",
            "[ 0.31015813  1.85794275  0.19040393 43.10869722]\n",
            "[ 0.3124966   1.84972337  0.19379296 44.09666234]\n",
            "[ 0.31450514  1.84266359  0.19670386 44.94524259]\n",
            "[ 0.31623032  1.83659983  0.19920409 45.67410278]\n",
            "[ 0.3177121   1.83139156  0.20135157 46.30013336]\n",
            "[ 0.31898483  1.82691808  0.20319608 46.8378418 ]\n",
            "[ 0.32007799  1.82307574  0.20478037 47.29968884]\n",
            "[ 0.32101694  1.81977549  0.20614113 47.69637722]\n",
            "[ 0.32182341  1.81694084  0.20730992 48.03709971]\n",
            "[ 0.3225161   1.81450612  0.20831381 48.32975211]\n",
            "[ 0.32311107  1.81241489  0.20917607 48.58111631]\n",
            "[ 0.32362209  1.8106187   0.20991668 48.79701734]\n",
            "[ 0.32406102  1.80907592  0.2105528  48.98245846]\n",
            "[ 0.32443803  1.8077508   0.21109918 49.14173703]\n",
            "[ 0.32476184  1.80661263  0.21156847 49.27854414]\n",
            "[ 0.32503997  1.80563503  0.21197155 49.39605012]\n",
            "[ 0.32527887  1.80479536  0.21231777 49.49697803]\n",
            "[ 0.32548405  1.80407415  0.21261514 49.58366675]\n",
            "[ 0.32566029  1.8034547   0.21287055 49.65812519]\n",
            "[ 0.32581167  1.80292263  0.21308994 49.72207882]\n",
            "[ 0.32594169  1.80246564  0.21327837 49.7770097 ]\n",
            "[ 0.32605336  1.80207311  0.21344021 49.82419077]\n",
            "[ 0.32614928  1.80173597  0.21357922 49.86471541]\n",
            "[ 0.32623167  1.80144639  0.21369862 49.89952272]\n",
            "[ 0.32630243  1.80119766  0.21380118 49.92941932]\n",
            "[ 0.32636321  1.80098403  0.21388927 49.95509802]\n",
            "[ 0.32641542  1.80080053  0.21396492 49.9771539 ]\n",
            "[ 0.32646026  1.80064293  0.21402991 49.99609808]\n",
            "[ 0.32649877  1.80050756  0.21408572 50.01236956]\n",
            "[ 0.32653185  1.80039128  0.21413367 50.02634541]\n",
            "[ 0.32656026  1.80029142  0.21417484 50.03834951]\n",
            "[ 0.32658467  1.80020564  0.21421021 50.04866004]\n",
            "[ 0.32660563  1.80013196  0.21424059 50.05751593]\n",
            "[ 0.32662363  1.80006868  0.21426668 50.06512241]\n",
            "[ 0.3266391   1.80001432  0.2142891  50.07165574]\n",
            "[ 0.32665238  1.79996764  0.21430834 50.07726734]\n",
            "[ 0.32666379  1.79992754  0.21432488 50.08208723]\n",
            "[ 0.32667359  1.7998931   0.21433908 50.08622712]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1U4_rXY7QL"
      },
      "source": [
        "After 1000 iterations it isn't even close, like it may never converge but by 10 000 it starts to look like the values are only changing a little.  I think this is part of the problem with gradient descent is that it just goes really slowly as you get close!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Biua8UZcQr"
      },
      "source": [
        "Maybe adding in some noise will help?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeHpbXyDYnH8",
        "outputId": "3b4c6fa6-4db7-4013-8340-8d36b3bde32e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "b = np.random.normal(0,5,4)#intitialize\n",
        "w = 0.001 #a weight for the descent\n",
        "\n",
        "for i in range(100000):\n",
        "  b = b - w*np.random.normal(0,1)*grad(b)/(i+1)**2\n",
        "  \n",
        "print(b)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.94595788  0.0774263  -0.34388532  0.52931364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imDOKOCQabeC"
      },
      "source": [
        "Doesn't seem too.  I added a feature to diminish the noise as the process has been running longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb-ndV77Yo6N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}